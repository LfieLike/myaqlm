{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43871e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quant/.conda/envs/AWQ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %env CUDA_VISIBLE_DEVICES=6\n",
    "# %env TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
    "# %env OMP_NUM_THREADS=16\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# 设置CUDA_VISIBLE_DEVICES环境变量\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import trange\n",
    "# import ipynbname  # pip install ipynbname\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "from typing import Optional,Union,List\n",
    "\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "from src.kmeans import find_nearest_cluster, fit_faiss_kmeans, fit_kmeans, fit_kmeans_1d\n",
    "from src.utils import ellipsis, maybe_script\n",
    "from src.aq import QuantizedWeight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantized_weight = QuantizedWeight(\n",
    "#     XTX=XTX, reference_weight=reference_weight, num_codebooks=num_codebooks,\n",
    "#     nbits_per_codebook=nbits_per_codebook, scale_nbits=scale_nbits, \n",
    "#     out_group_size=out_group_size, in_group_size=in_group_size,\n",
    "#     verbose=True, max_iter=init_max_iter,   # faster init, not tested\n",
    "# )\n",
    "num_codebooks = 1\n",
    "nbits_per_codebook = 8\n",
    "out_group_size = 1\n",
    "in_group_size = 4\n",
    "# batch_size = 16384\n",
    "# beam_size = 1\n",
    "# beam_search_epochs = 100\n",
    "# sparsity_regularizer = 0\n",
    "# print_frequency = 10\n",
    "scale_nbits = 0    # 0 means no scales, 16 means no compression;\n",
    "# codebook_values_nbits = 16  # less than 16 means we quantize codebooks as well\n",
    "init_max_iter = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([262144, 4])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/quant/AQLM-main/notebooks/aq_simple.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m4096\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4096\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m16\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m fit_kmeans(x,\u001b[39m256\u001b[39;49m,\u001b[39m1000\u001b[39;49m)\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def fit_kmeans(\n",
    "    data: torch.Tensor,\n",
    "    k: int,\n",
    "    max_iter: int = 1000,\n",
    "    check_every: int = 10,\n",
    "    rtol: float = 1e-06,\n",
    "    atol: float = 1e-08,\n",
    "    greedy_init: bool = False,\n",
    "    block_size_vals: int = 2**30,\n",
    "    devices: Optional[List[torch.device]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    :param data: [nsamples, dim]\n",
    "    :param k: number of centroids\n",
    "    :param max_iter: run at most this many iterations\n",
    "    :param check_every: check for convergence (allclose(new_centroids, old_centroids)) once in this many steps\n",
    "    :param rtol: early stopping relative tolerance for centroids\n",
    "    :param atol: early stopping absolute tolerance for centroids\n",
    "    :param greedy_init: if True, init by greedily selecting the point that is farthest from any cluster\n",
    "        if False (default), initialize with random points using pytorch global RNG\n",
    "    :param block_size_vals: how many dot products to compute at a time\n",
    "    :param devices: if specified, run kmeans in data-parallel mode across these devices\n",
    "    :return: (clusters float[k, dim], data_indices int[nsamples], reconstructed_data: float[nsamples, dim])\n",
    "    \"\"\"\n",
    "    if devices is None:\n",
    "        devices = [data.device]\n",
    "\n",
    "    if greedy_init:\n",
    "        clusters = _kmeans_greedy_init(data, k)\n",
    "    else:\n",
    "        clusters = data[torch.randperm(data.shape[0])[:k], :]  # [k, dim]\n",
    "\n",
    "    block_size = block_size_vals // k\n",
    "    shard_size = (len(data) - 1) // len(devices) + 1\n",
    "    data = [\n",
    "        data[gi * shard_size : (gi + 1) * shard_size].to(devices[gi], non_blocking=True) for gi in range(len(devices))\n",
    "    ]\n",
    "    nearest_indices = [torch.empty(len(data[gi]), dtype=torch.int64, device=devices[gi]) for gi in range(len(devices))]\n",
    "    clusters = [clusters.to(device, non_blocking=True) for device in devices]\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        for block_start in range(0, shard_size, block_size):\n",
    "            for gi in range(len(devices)):\n",
    "                nearest_indices[gi][block_start : block_start + block_size] = torch.addmm(\n",
    "                    torch.bmm(clusters[gi][:, None, :], clusters[gi][:, :, None]).flatten(),\n",
    "                    data[gi][block_start : block_start + block_size],\n",
    "                    clusters[gi].T,\n",
    "                    beta=-0.5,\n",
    "                ).argmax(1)\n",
    "            # note: the above formula equals to - 0.5 || data[:, None, :] - clusters[None, :, :] || ^ 2 + const\n",
    "\n",
    "        if len(devices) == 1:\n",
    "            new_clusters = [\n",
    "                clusters[0]\n",
    "                .clone()\n",
    "                .index_reduce_(dim=0, index=nearest_indices[0], source=data[0], reduce=\"mean\", include_self=False)\n",
    "            ]\n",
    "        else:\n",
    "            cluster_sums = [\n",
    "                torch.zeros_like(clusters[gi])\n",
    "                .index_add(dim=0, index=nearest_indices[gi], source=data[gi])\n",
    "                .to(devices[0], non_blocking=True)\n",
    "                for gi in range(len(devices))\n",
    "            ]\n",
    "            cluster_counts = [\n",
    "                torch.bincount(nearest_indices[gi], minlength=k).to(devices[0], non_blocking=True)\n",
    "                for gi in range(len(devices))\n",
    "            ]\n",
    "            for gi in range(1, len(devices)):\n",
    "                cluster_sums[0] += cluster_sums[gi]\n",
    "                cluster_counts[0] += cluster_counts[gi]\n",
    "\n",
    "            new_clusters = [cluster_sums[0] / cluster_counts[0].unsqueeze(1).clamp_min(1)]\n",
    "            new_clusters[0] += (cluster_counts[0].unsqueeze(1) == 0) * clusters[0]\n",
    "            for gi in range(1, len(devices)):\n",
    "                new_clusters.append(new_clusters[0].to(devices[gi], non_blocking=True))\n",
    "\n",
    "        if i % check_every == 0:\n",
    "            if torch.allclose(new_clusters[0], clusters[0], rtol=rtol, atol=atol):\n",
    "                break\n",
    "        clusters = new_clusters\n",
    "    for block_start in range(0, shard_size, block_size):\n",
    "        for gi in range(len(devices)):\n",
    "            nearest_indices[gi][block_start : block_start + block_size] = torch.addmm(\n",
    "                torch.bmm(clusters[gi][:, None, :], clusters[gi][:, :, None]).flatten(),\n",
    "                data[gi][block_start : block_start + block_size],\n",
    "                clusters[gi].T,\n",
    "                beta=-0.5,\n",
    "            ).argmax(1)\n",
    "\n",
    "    clusters = clusters[0]\n",
    "    nearest_indices = torch.cat([nearest_indices[gi].to(devices[0]) for gi in range(len(devices))], dim=0)\n",
    "    reconstructed_data = clusters[nearest_indices]\n",
    "    return clusters, nearest_indices, reconstructed_data\n",
    "x = torch.randn(4096*4096//16//4, 4, device='cuda')\n",
    "print(x.shape)\n",
    "clusters, nearest_indices, reconstructed_data = fit_kmeans(x,256,1000)\n",
    "reconstructed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "initializing with kmeans: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') \n",
    "x = torch.load(\"/home/quant/test.pth\",map_location=device)\n",
    "reference_weight = x\n",
    "num_codebooks = 1\n",
    "nbits_per_codebook = 8\n",
    "out_group_size = 1\n",
    "in_group_size = 4\n",
    "batch_size = 16384\n",
    "beam_size = 1\n",
    "beam_search_epochs = 100\n",
    "sparsity_regularizer = 0\n",
    "print_frequency = 10\n",
    "scale_nbits = 0    # 0 means no scales, 16 means no compression;\n",
    "# codebook_values_nbits = 16  # less than 16 means we quantize codebooks as well\n",
    "init_max_iter = 500\n",
    "quantized_weight_list = []\n",
    "slip_x = x.split(x.shape[0]//16, dim=0)\n",
    "for tensor in slip_x:\n",
    "    XTX =torch.eye(tensor.shape[-1])\n",
    "    quantized_weight = QuantizedWeight(\n",
    "        XTX = XTX,\n",
    "        reference_weight=tensor, num_codebooks=num_codebooks, \n",
    "        nbits_per_codebook=nbits_per_codebook, scale_nbits=scale_nbits, \n",
    "        out_group_size=out_group_size, in_group_size=in_group_size, \n",
    "        verbose=True, max_iter=init_max_iter,  \n",
    "    )\n",
    "    quantized_weight_list.append(quantized_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.log({\"Avg_bits\": quantized_weight.estimate_nbits_per_parameter()})\n",
    "# print(\"AVG bits:\", quantized_weight.estimate_nbits_per_parameter())\n",
    "# print(quantized_weight.parameters())\n",
    "param_list = [param for model in quantized_weight_list for param in model.parameters()]\n",
    "\n",
    "opt = torch.optim.Adam(param_list, lr=1e-4, betas=(0.0, 0.95), amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0735402556\t time_on_epoch 0 = 0.011769775301218033\n",
      "loss=nan\t time_on_epoch 1 = 0.011287960223853588\n",
      "loss=nan\t time_on_epoch 2 = 0.011282090097665787\n",
      "loss=nan\t time_on_epoch 3 = 0.011256150901317596\n",
      "loss=nan\t time_on_epoch 4 = 0.011364913545548916\n",
      "loss=nan\t time_on_epoch 5 = 0.01133693102747202\n",
      "loss=nan\t time_on_epoch 6 = 0.01136601623147726\n",
      "loss=nan\t time_on_epoch 7 = 0.011247285641729832\n",
      "loss=nan\t time_on_epoch 8 = 0.009272407740354538\n",
      "loss=nan\t time_on_epoch 9 = 0.009311560541391373\n",
      "loss=nan\t time_on_epoch 10 = 0.009423838928341866\n",
      "loss=nan\t time_on_epoch 11 = 0.009344280697405338\n",
      "loss=nan\t time_on_epoch 12 = 0.009240148589015007\n",
      "loss=nan\t time_on_epoch 13 = 0.009282426908612251\n",
      "loss=nan\t time_on_epoch 14 = 0.009432234801352024\n",
      "loss=nan\t time_on_epoch 15 = 0.009226513095200062\n",
      "loss=nan\t time_on_epoch 16 = 0.00909573957324028\n",
      "loss=nan\t time_on_epoch 17 = 0.00908541027456522\n",
      "loss=nan\t time_on_epoch 18 = 0.009235299192368984\n",
      "loss=nan\t time_on_epoch 19 = 0.009088948369026184\n",
      "loss=nan\t time_on_epoch 20 = 0.009074601344764233\n",
      "loss=nan\t time_on_epoch 21 = 0.009155882522463799\n",
      "loss=nan\t time_on_epoch 22 = 0.009166471660137177\n",
      "loss=nan\t time_on_epoch 23 = 0.009116468019783497\n",
      "loss=nan\t time_on_epoch 24 = 0.009089718572795391\n",
      "loss=nan\t time_on_epoch 25 = 0.009148146957159042\n",
      "loss=nan\t time_on_epoch 26 = 0.009046929888427258\n",
      "loss=nan\t time_on_epoch 27 = 0.009121827781200409\n",
      "loss=nan\t time_on_epoch 28 = 0.009094597771763802\n",
      "loss=nan\t time_on_epoch 29 = 0.009144119918346405\n",
      "loss=nan\t time_on_epoch 30 = 0.0092623895034194\n",
      "loss=nan\t time_on_epoch 31 = 0.00913842860609293\n",
      "loss=nan\t time_on_epoch 32 = 0.009061487391591072\n",
      "loss=nan\t time_on_epoch 33 = 0.009193400852382183\n",
      "loss=nan\t time_on_epoch 34 = 0.009147866629064083\n",
      "loss=nan\t time_on_epoch 35 = 0.009121968410909176\n",
      "loss=nan\t time_on_epoch 36 = 0.009088806807994843\n",
      "loss=nan\t time_on_epoch 37 = 0.009127108380198479\n",
      "loss=nan\t time_on_epoch 38 = 0.00909342523664236\n",
      "loss=nan\t time_on_epoch 39 = 0.00910072959959507\n",
      "loss=nan\t time_on_epoch 40 = 0.009082023985683918\n",
      "loss=nan\t time_on_epoch 41 = 0.00910322368144989\n",
      "loss=nan\t time_on_epoch 42 = 0.009069662541151047\n",
      "loss=nan\t time_on_epoch 43 = 0.009122048504650593\n",
      "loss=nan\t time_on_epoch 44 = 0.009066023863852024\n",
      "loss=nan\t time_on_epoch 45 = 0.009105748496949673\n",
      "loss=nan\t time_on_epoch 46 = 0.009206164628267288\n",
      "loss=nan\t time_on_epoch 47 = 0.009108994156122208\n",
      "loss=nan\t time_on_epoch 48 = 0.009107420220971107\n",
      "loss=nan\t time_on_epoch 49 = 0.009098383598029613\n",
      "loss=nan\t time_on_epoch 50 = 0.009152214974164963\n",
      "loss=nan\t time_on_epoch 51 = 0.009119994007050991\n",
      "loss=nan\t time_on_epoch 52 = 0.00907282717525959\n",
      "loss=nan\t time_on_epoch 53 = 0.00908664334565401\n",
      "loss=nan\t time_on_epoch 54 = 0.009097271598875523\n",
      "loss=nan\t time_on_epoch 55 = 0.009131104685366154\n",
      "loss=nan\t time_on_epoch 56 = 0.009073147550225258\n",
      "loss=nan\t time_on_epoch 57 = 0.00914870947599411\n",
      "loss=nan\t time_on_epoch 58 = 0.009090499952435493\n",
      "loss=nan\t time_on_epoch 59 = 0.009146303869783878\n",
      "loss=nan\t time_on_epoch 60 = 0.009066665545105934\n",
      "loss=nan\t time_on_epoch 61 = 0.009127628989517689\n",
      "loss=nan\t time_on_epoch 62 = 0.009083026088774204\n",
      "loss=nan\t time_on_epoch 63 = 0.009097723290324211\n",
      "loss=nan\t time_on_epoch 64 = 0.00909493863582611\n",
      "loss=nan\t time_on_epoch 65 = 0.009149931371212006\n",
      "loss=nan\t time_on_epoch 66 = 0.009122909978032112\n",
      "loss=nan\t time_on_epoch 67 = 0.009098854847252369\n",
      "loss=nan\t time_on_epoch 68 = 0.00905902124941349\n",
      "loss=nan\t time_on_epoch 69 = 0.009128379635512829\n",
      "loss=nan\t time_on_epoch 70 = 0.00908739399164915\n",
      "loss=nan\t time_on_epoch 71 = 0.009133289568126202\n",
      "loss=nan\t time_on_epoch 72 = 0.009046457707881927\n",
      "loss=nan\t time_on_epoch 73 = 0.009144540876150131\n",
      "loss=nan\t time_on_epoch 74 = 0.009178943932056427\n",
      "loss=nan\t time_on_epoch 75 = 0.009107311256229877\n",
      "loss=nan\t time_on_epoch 76 = 0.009065182879567146\n",
      "loss=nan\t time_on_epoch 77 = 0.009118280373513699\n",
      "loss=nan\t time_on_epoch 78 = 0.00906667672097683\n",
      "loss=nan\t time_on_epoch 79 = 0.00905617605894804\n",
      "loss=nan\t time_on_epoch 80 = 0.009061415679752827\n",
      "loss=nan\t time_on_epoch 81 = 0.009076855145394802\n",
      "loss=nan\t time_on_epoch 82 = 0.009140552952885628\n",
      "loss=nan\t time_on_epoch 83 = 0.009310758672654629\n",
      "loss=nan\t time_on_epoch 84 = 0.009095558896660805\n",
      "loss=nan\t time_on_epoch 85 = 0.009088335558772087\n",
      "loss=nan\t time_on_epoch 86 = 0.009082004427909851\n",
      "loss=nan\t time_on_epoch 87 = 0.009111759252846241\n",
      "loss=nan\t time_on_epoch 88 = 0.009070022031664848\n",
      "loss=nan\t time_on_epoch 89 = 0.009103454649448395\n",
      "loss=nan\t time_on_epoch 90 = 0.009143739007413387\n",
      "loss=nan\t time_on_epoch 91 = 0.009080059826374054\n",
      "loss=nan\t time_on_epoch 92 = 0.00908912718296051\n",
      "loss=nan\t time_on_epoch 93 = 0.009306350722908974\n",
      "loss=nan\t time_on_epoch 94 = 0.009064051322638988\n",
      "loss=nan\t time_on_epoch 95 = 0.009253822267055511\n",
      "loss=nan\t time_on_epoch 96 = 0.009118601679801941\n",
      "loss=nan\t time_on_epoch 97 = 0.009165690280497074\n",
      "loss=nan\t time_on_epoch 98 = 0.009065013378858566\n",
      "loss=nan\t time_on_epoch 99 = 0.009255806915462017\n",
      "loss=nan\t time_on_epoch 100 = 0.009132417850196362\n",
      "loss=nan\t time_on_epoch 101 = 0.00911151897162199\n",
      "loss=nan\t time_on_epoch 102 = 0.009160730987787247\n",
      "loss=nan\t time_on_epoch 103 = 0.009200243279337883\n",
      "loss=nan\t time_on_epoch 104 = 0.009091641753911972\n",
      "loss=nan\t time_on_epoch 105 = 0.00907310750335455\n",
      "loss=nan\t time_on_epoch 106 = 0.009068429470062256\n",
      "loss=nan\t time_on_epoch 107 = 0.009088415652513504\n",
      "loss=nan\t time_on_epoch 108 = 0.009123580530285835\n",
      "loss=nan\t time_on_epoch 109 = 0.009079008363187313\n",
      "loss=nan\t time_on_epoch 110 = 0.009149870835244656\n",
      "loss=nan\t time_on_epoch 111 = 0.009318343363702297\n",
      "loss=nan\t time_on_epoch 112 = 0.009080822579562664\n",
      "loss=nan\t time_on_epoch 113 = 0.00909765250980854\n",
      "loss=nan\t time_on_epoch 114 = 0.009044154547154903\n",
      "loss=nan\t time_on_epoch 115 = 0.009090369567275047\n",
      "loss=nan\t time_on_epoch 116 = 0.009049584157764912\n",
      "loss=nan\t time_on_epoch 117 = 0.009065462276339531\n",
      "loss=nan\t time_on_epoch 118 = 0.009125163778662682\n",
      "loss=nan\t time_on_epoch 119 = 0.00908396765589714\n",
      "loss=nan\t time_on_epoch 120 = 0.009088024497032166\n",
      "loss=nan\t time_on_epoch 121 = 0.009128370322287083\n",
      "loss=nan\t time_on_epoch 122 = 0.009075571782886982\n",
      "loss=nan\t time_on_epoch 123 = 0.009114515036344528\n",
      "loss=nan\t time_on_epoch 124 = 0.00905071571469307\n",
      "loss=nan\t time_on_epoch 125 = 0.009133821353316307\n",
      "loss=nan\t time_on_epoch 126 = 0.009082053788006306\n",
      "loss=nan\t time_on_epoch 127 = 0.009078117087483406\n",
      "loss=nan\t time_on_epoch 128 = 0.009061235934495926\n",
      "loss=nan\t time_on_epoch 129 = 0.009095679968595505\n",
      "loss=nan\t time_on_epoch 130 = 0.00904875248670578\n",
      "loss=nan\t time_on_epoch 131 = 0.009117290377616882\n",
      "loss=nan\t time_on_epoch 132 = 0.009069571271538734\n",
      "loss=nan\t time_on_epoch 133 = 0.009089106693863869\n",
      "loss=nan\t time_on_epoch 134 = 0.009064421989023685\n",
      "loss=nan\t time_on_epoch 135 = 0.009099636226892471\n",
      "loss=nan\t time_on_epoch 136 = 0.009094216860830784\n",
      "loss=nan\t time_on_epoch 137 = 0.009105388075113297\n",
      "loss=nan\t time_on_epoch 138 = 0.009068097919225693\n",
      "loss=nan\t time_on_epoch 139 = 0.009161001071333885\n",
      "loss=nan\t time_on_epoch 140 = 0.009077625349164009\n",
      "loss=nan\t time_on_epoch 141 = 0.009108824655413628\n",
      "loss=nan\t time_on_epoch 142 = 0.009093035012483597\n",
      "loss=nan\t time_on_epoch 143 = 0.009084679186344147\n",
      "loss=nan\t time_on_epoch 144 = 0.00907884817570448\n",
      "loss=nan\t time_on_epoch 145 = 0.009193881414830685\n",
      "loss=nan\t time_on_epoch 146 = 0.009069892577826977\n",
      "loss=nan\t time_on_epoch 147 = 0.009127408266067505\n",
      "loss=nan\t time_on_epoch 148 = 0.009091692045331001\n",
      "loss=nan\t time_on_epoch 149 = 0.00911505427211523\n",
      "loss=nan\t time_on_epoch 150 = 0.009088647551834583\n",
      "loss=nan\t time_on_epoch 151 = 0.009124322794377804\n",
      "loss=nan\t time_on_epoch 152 = 0.009143419563770294\n",
      "loss=nan\t time_on_epoch 153 = 0.009071284905076027\n",
      "loss=nan\t time_on_epoch 154 = 0.009053199551999569\n",
      "loss=nan\t time_on_epoch 155 = 0.009089487604796886\n",
      "loss=nan\t time_on_epoch 156 = 0.009083637967705727\n",
      "loss=nan\t time_on_epoch 157 = 0.009288798086345196\n",
      "loss=nan\t time_on_epoch 158 = 0.009092333726584911\n",
      "loss=nan\t time_on_epoch 159 = 0.00909092091023922\n",
      "loss=nan\t time_on_epoch 160 = 0.009174054488539696\n",
      "loss=nan\t time_on_epoch 161 = 0.009111148305237293\n",
      "loss=nan\t time_on_epoch 162 = 0.009047280065715313\n",
      "loss=nan\t time_on_epoch 163 = 0.009090369567275047\n",
      "loss=nan\t time_on_epoch 164 = 0.009174425154924393\n",
      "loss=nan\t time_on_epoch 165 = 0.009156032465398312\n",
      "loss=nan\t time_on_epoch 166 = 0.009074660949409008\n",
      "loss=nan\t time_on_epoch 167 = 0.009169827215373516\n",
      "loss=nan\t time_on_epoch 168 = 0.009086012840270996\n",
      "loss=nan\t time_on_epoch 169 = 0.009233935736119747\n",
      "loss=nan\t time_on_epoch 170 = 0.009060795418918133\n",
      "loss=nan\t time_on_epoch 171 = 0.009111229330301285\n",
      "loss=nan\t time_on_epoch 172 = 0.009177161380648613\n",
      "loss=nan\t time_on_epoch 173 = 0.009060284122824669\n",
      "loss=nan\t time_on_epoch 174 = 0.009109917096793652\n",
      "loss=nan\t time_on_epoch 175 = 0.009113222360610962\n",
      "loss=nan\t time_on_epoch 176 = 0.009273790754377842\n",
      "loss=nan\t time_on_epoch 177 = 0.009152916260063648\n",
      "loss=nan\t time_on_epoch 178 = 0.009079940617084503\n",
      "loss=nan\t time_on_epoch 179 = 0.009097922593355179\n",
      "loss=nan\t time_on_epoch 180 = 0.009259795770049095\n",
      "loss=nan\t time_on_epoch 181 = 0.00932359229773283\n",
      "loss=nan\t time_on_epoch 182 = 0.009102282114326954\n",
      "loss=nan\t time_on_epoch 183 = 0.00908308569341898\n",
      "loss=nan\t time_on_epoch 184 = 0.009211504831910133\n",
      "loss=nan\t time_on_epoch 185 = 0.009093133732676506\n",
      "loss=nan\t time_on_epoch 186 = 0.009076584130525589\n",
      "loss=nan\t time_on_epoch 187 = 0.009082726202905178\n",
      "loss=nan\t time_on_epoch 188 = 0.009168204851448536\n",
      "loss=nan\t time_on_epoch 189 = 0.00909468811005354\n",
      "loss=nan\t time_on_epoch 190 = 0.009091783314943314\n",
      "loss=nan\t time_on_epoch 191 = 0.009094487875699997\n",
      "loss=nan\t time_on_epoch 192 = 0.009065063670277596\n",
      "loss=nan\t time_on_epoch 193 = 0.009113471955060959\n",
      "loss=nan\t time_on_epoch 194 = 0.00904997531324625\n",
      "loss=nan\t time_on_epoch 195 = 0.009052890352904797\n",
      "loss=nan\t time_on_epoch 196 = 0.00906585343182087\n",
      "loss=nan\t time_on_epoch 197 = 0.00909719243645668\n",
      "loss=nan\t time_on_epoch 198 = 0.009089058265089989\n",
      "loss=nan\t time_on_epoch 199 = 0.009154328145086765\n",
      "loss=nan\t time_on_epoch 200 = 0.00912259891629219\n",
      "loss=nan\t time_on_epoch 201 = 0.009099096059799194\n",
      "loss=nan\t time_on_epoch 202 = 0.00910172052681446\n",
      "loss=nan\t time_on_epoch 203 = 0.009146073833107948\n",
      "loss=nan\t time_on_epoch 204 = 0.009065823629498482\n",
      "loss=nan\t time_on_epoch 205 = 0.009302202612161636\n",
      "loss=nan\t time_on_epoch 206 = 0.009075552225112915\n",
      "loss=nan\t time_on_epoch 207 = 0.00912504456937313\n",
      "loss=nan\t time_on_epoch 208 = 0.009232393465936184\n",
      "loss=nan\t time_on_epoch 209 = 0.009143608622252941\n",
      "loss=nan\t time_on_epoch 210 = 0.009079460054636002\n",
      "loss=nan\t time_on_epoch 211 = 0.009141054004430771\n",
      "loss=nan\t time_on_epoch 212 = 0.009133400395512581\n",
      "loss=nan\t time_on_epoch 213 = 0.009148849174380302\n",
      "loss=nan\t time_on_epoch 214 = 0.009100419469177723\n",
      "loss=nan\t time_on_epoch 215 = 0.00907004252076149\n",
      "loss=nan\t time_on_epoch 216 = 0.009266367182135582\n",
      "loss=nan\t time_on_epoch 217 = 0.009158696979284286\n",
      "loss=nan\t time_on_epoch 218 = 0.00912479404360056\n",
      "loss=nan\t time_on_epoch 219 = 0.009079750627279282\n",
      "loss=nan\t time_on_epoch 220 = 0.009130635298788548\n",
      "loss=nan\t time_on_epoch 221 = 0.009098434820771217\n",
      "loss=nan\t time_on_epoch 222 = 0.0091160973533988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/quant/AQLM-main/notebooks/aq_simple.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# if epoch % print_frequency == 0:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss=\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39;49mitem()\u001b[39m:\u001b[39;00m\u001b[39m.10f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtime_on_epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mperf_counter()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# if (epoch + 1) % beam_search_epochs == 0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#     for quantized_weight in quantized_weight_list:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#         quantized_weight.beam_search_update_codes_(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#             sparsity_rate = ((quantized_weight.codes == 0).sum() / quantized_weight.codes.numel()).item()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.242.193/home/quant/AQLM-main/notebooks/aq_simple.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#             print(f\"Sparsity rate {sparsity_rate:.5f}\")\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_frequency = 20\n",
    "for epoch in range(1000):\n",
    "    XTX = torch.eye(4096).cuda()\n",
    "    start = time.perf_counter()\n",
    "    now_weight = torch.cat([cnt_weight() for cnt_weight in quantized_weight_list],dim = 0)\n",
    "    # print(now_weight.shape)\n",
    "    delta_weight = (now_weight - reference_weight).double()\n",
    "    loss = (delta_weight).flatten() @ delta_weight.flatten() / len(delta_weight)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    # if epoch % print_frequency == 0:\n",
    "    print(f\"loss={loss.item():.10f}\\t\",\n",
    "            f\"time_on_epoch {epoch} = {time.perf_counter() - start}\")\n",
    "    # if (epoch + 1) % beam_search_epochs == 0:\n",
    "    #     for quantized_weight in quantized_weight_list:\n",
    "    #         quantized_weight.beam_search_update_codes_(\n",
    "    #             XTX, reference_weight, beam_size=beam_size, sparsity_regularizer=sparsity_regularizer,\n",
    "    #             dim_rng=random.Random(), verbose=True)\n",
    "\n",
    "    #         if sparsity_regularizer != 0:\n",
    "    #             sparsity_rate = ((quantized_weight.codes == 0).sum() / quantized_weight.codes.numel()).item()\n",
    "    #             print(f\"Sparsity rate {sparsity_rate:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4096, 64, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_weight.codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
